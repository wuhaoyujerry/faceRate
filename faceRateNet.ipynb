{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python3/3.6.3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import os.path\n",
    "import re\n",
    "import sys\n",
    "import tarfile\n",
    "import pandas\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from scipy.misc import imsave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#parameters \n",
    "input_width = 299\n",
    "input_height = 299\n",
    "input_depth = 3\n",
    "input_mean = 128\n",
    "input_std = 128\n",
    "step = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_jpeg_decoding(input_width, input_height, input_depth, input_mean, input_std):\n",
    "    \"\"\"Adds operations that perform JPEG decoding and resizing to the graph..\n",
    "    \n",
    "    Args:\n",
    "      input_width: Desired width of the image fed into the recognizer graph.\n",
    "      input_height: Desired width of the image fed into the recognizer graph.\n",
    "      input_depth: Desired channels of the image fed into the recognizer graph.\n",
    "      input_mean: Pixel value that should be zero in the image for the graph.\n",
    "      input_std: How much to divide the pixel values by before recognition.\n",
    "    \n",
    "    Returns:\n",
    "      Tensors for the node to feed JPEG data into, and the output of the\n",
    "        preprocessing steps.\n",
    "    \"\"\"\n",
    "    jpeg_data = tf.placeholder(tf.string, name='DecodeJPGInput')\n",
    "    decoded_image = tf.image.decode_jpeg(jpeg_data, channels=input_depth)\n",
    "    decoded_image_as_float = tf.cast(decoded_image, dtype=tf.float32)\n",
    "    decoded_image_4d = tf.expand_dims(decoded_image_as_float, 0)\n",
    "    resize_shape = tf.stack([input_height, input_width])\n",
    "    resize_shape_as_int = tf.cast(resize_shape, dtype=tf.int32)\n",
    "    resized_image = tf.image.resize_bilinear(decoded_image_4d,\n",
    "                                             resize_shape_as_int)\n",
    "    offset_image = tf.subtract(resized_image, input_mean)\n",
    "    mul_image = tf.multiply(offset_image, 1.0 / input_std)\n",
    "    return jpeg_data, mul_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_graph():\n",
    "    \"\"\"Creates a graph from saved GraphDef file and returns a saver.\"\"\"\n",
    "    # Creates graph from saved graph_def.pb.\n",
    "    with tf.gfile.FastGFile('classify_image_graph_def.pb', 'rb') as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "        _ = tf.import_graph_def(graph_def, name='')\n",
    "    \n",
    "    with tf.name_scope(\"new_last_layer\"):\n",
    "        weight = tf.get_variable('weight', [2048, 2])\n",
    "        bias = tf.get_variable('bias', [2])\n",
    "        bottle_neck = tf.placeholder(tf.float32, [None, 2048])\n",
    "        ground_truth = tf.placeholder(tf.float32, [None, 2])\n",
    "        \n",
    "        prediction = tf.nn.softmax(tf.add(tf.matmul(bottle_neck, weight), bias), name='prediction')\n",
    "        loss = tf.reduce_mean(tf.losses.log_loss(ground_truth, prediction), name='loss')\n",
    "        \n",
    "    with tf.name_scope('train'):\n",
    "        optimizer = tf.train.GradientDescentOptimizer(step)\n",
    "        train_step = optimizer.minimize(loss)\n",
    "        \n",
    "    return bottle_neck, ground_truth, prediction, loss, train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_train_data(label_file, image_folder, valid_portion):\n",
    "    df = pandas.read_csv(label_file).head(n=5000)\n",
    "    print('Read %d records.' % len(df))\n",
    "    df['image_path'] = [str(image_folder) + '/' + str(id) + '.jpg' for id in df['uid'].tolist()]\n",
    "    valid_data = df[df.apply(lambda x: x['uid'] % 100 < valid_portion, axis=1)]\n",
    "    train_data = df[~df.index.isin(valid_data.index)]\n",
    "    return train_data, valid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_test_data(label_file, image_folder):\n",
    "    df = pandas.read_csv(label_file).head(n=1000)\n",
    "    print('Read %d records.' % len(df))\n",
    "    df['image_path'] = [str(image_folder) + '/' + str(id) + '.jpg' for id in df['uid'].tolist()]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def df_2_dict(df_data):\n",
    "    score = np.array(df_data['attractive_score'].tolist())\n",
    "    return {\n",
    "            'image_path': df_data['image_path'].tolist(),\n",
    "            'score': np.column_stack((score, 1 - score))\n",
    "    }\n",
    "\n",
    "def generate_batch_data(train_data, batch_num):\n",
    "    train_size = len(train_data)\n",
    "    batch_size = int(train_size / batch_num)\n",
    "    result = []\n",
    "    perm = np.random.permutation(train_size)\n",
    "    for batch in range(batch_num):\n",
    "        indices = perm[batch * batch_size: (batch + 1) * batch_size]\n",
    "        batch_data = train_data.iloc[indices]\n",
    "        result.append(df_2_dict(batch_data))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extra_feature(data, sess, mul_image, jpeg_data, feature_tensor, input_tensor):\n",
    "    #parpare data\n",
    "    feature = []\n",
    "    score = []\n",
    "    for idx in range(len(data['image_path'])):\n",
    "        image_path = data['image_path'][idx]\n",
    "        if tf.gfile.Exists(image_path):\n",
    "            try:\n",
    "                image_data = tf.gfile.FastGFile(image_path, 'rb').read()\n",
    "                decoded_data = sess.run(mul_image, {jpeg_data: image_data})\n",
    "                feature_score = sess.run(feature_tensor, {input_tensor: decoded_data})\n",
    "                feature.append(np.squeeze(feature_score))\n",
    "                score.append(data['score'][idx, :])\n",
    "            except:\n",
    "                pass\n",
    "    feature = np.array(feature)\n",
    "    score = np.array(score)\n",
    "    return feature, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def start_session():\n",
    "    # Creates graph from saved GraphDef.\n",
    "    bottle_neck, ground_truth, prediction, loss, train_step = create_graph()\n",
    "    jpeg_data, mul_image = add_jpeg_decoding(input_width, input_height, input_depth, input_mean, input_std)\n",
    "    saver = tf.train.Saver()\n",
    "    sess = tf.Session()\n",
    "    input_tensor = sess.graph.get_tensor_by_name('Mul:0')\n",
    "    feature_tensor = sess.graph.get_tensor_by_name('pool_3/_reshape:0')\n",
    "    return (bottle_neck, ground_truth, prediction, loss, train_step,\n",
    "            jpeg_data, mul_image,\n",
    "            input_tensor, feature_tensor,\n",
    "            sess, saver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_network(label_file, image_folder, valid_portion = 10, batch_num = 5, epoch = 10):\n",
    "    train_data, valid_data = read_train_data(label_file, image_folder, valid_portion)\n",
    "    valid_data = df_2_dict(valid_data)\n",
    "        \n",
    "    cache = {}\n",
    "\n",
    "    #prepare valid data\n",
    "    valid_feature, valid_score = extra_feature(valid_data, \n",
    "                                              sess, \n",
    "                                              mul_image, jpeg_data, \n",
    "                                              feature_tensor, input_tensor)\n",
    "\n",
    "    # train the network\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    count = 0\n",
    "    for ep in range(epoch):\n",
    "        for batch_data in generate_batch_data(train_data, batch_num):\n",
    "            feature, score = extra_feature(batch_data, \n",
    "                                           sess, \n",
    "                                           mul_image, jpeg_data, \n",
    "                                           feature_tensor, input_tensor)\n",
    "            sess.run(train_step, {bottle_neck: feature, ground_truth: score})\n",
    "            curr_loss = sess.run(loss, {bottle_neck: feature, ground_truth: score})\n",
    "            print('step %d loss %f' % (count, curr_loss))\n",
    "            count += 1\n",
    "\n",
    "        valid_loss = sess.run(loss, {bottle_neck: valid_feature, ground_truth: valid_score})\n",
    "        print('epoch %d loss %f'%(ep, valid_loss))\n",
    "    saver.save(sess, \"checkpoint/model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(label_file, image_folder, test = True):\n",
    "    # Creates graph from saved GraphDef.\n",
    "    test_data = read_test_data(label_file, image_folder)\n",
    "    test_data = df_2_dict(test_data)\n",
    "    \n",
    "    saver.restore(sess, \"checkpoint/model.ckpt\")\n",
    "\n",
    "    test_feature, test_score = extra_feature(test_data, \n",
    "                                            sess, \n",
    "                                            mul_image, jpeg_data, \n",
    "                                            feature_tensor, input_tensor)\n",
    "    if test:\n",
    "        curr_loss = sess.run(loss, {bottle_neck: test_feature, ground_truth: test_score})\n",
    "        print('Test loss: %f' % curr_loss)\n",
    "    else:\n",
    "        predict = sess.run(prediction, {bottle_neck: test_feature})\n",
    "        return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 5000 records.\n",
      "step 0 loss 0.629265\n"
     ]
    }
   ],
   "source": [
    "(bottle_neck, ground_truth, prediction, loss, train_step,\n",
    "            jpeg_data, mul_image,\n",
    "            input_tensor, feature_tensor,\n",
    "            sess, saver) = start_session()\n",
    "train_network('chunk2_uid_score.csv', 'chunk_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict('chunk2_uid_score.csv', 'chunk_2')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
